{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqBaQOtHVXhF"
      },
      "source": [
        "\"\"\"\n",
        "Authors:\n",
        "Nawras Abbas    315085043\n",
        "Michael Bikman  317920999\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Bernoulli\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import torch.nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "LOG_ENABLED = True\n",
        "\n",
        "log = print\n",
        "if LOG_ENABLED:\n",
        "    log = logging.info\n",
        "\n",
        "# current time\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/project'\n",
        "TRAIN_DATA_FILE = r\"mini-imagenet-cache-train.pkl\"\n",
        "VALID_DATA_FILE = r\"mini-imagenet-cache-val.pkl\"\n",
        "TEST_DATA_FILE = r\"mini-imagenet-cache-test.pkl\"\n",
        "\n",
        "DEVICE = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "\n",
        "\n",
        "\n",
        "# ********************************************* resnet12 *********************************************\n",
        "# ********************************************* resnet12 *********************************************\n",
        "# ********************************************* resnet12 *********************************************\n",
        "# ********************************************* resnet12 *********************************************\n",
        "# ********************************************* resnet12 *********************************************\n",
        "\n",
        "\n",
        "\n",
        "class DropBlock(nn.Module):\n",
        "    def __init__(self, block_size):\n",
        "        super(DropBlock, self).__init__()\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, x, gamma):\n",
        "        # shape: (bsize, channels, height, width)\n",
        "\n",
        "        if self.training:\n",
        "            batch_size, channels, height, width = x.shape\n",
        "\n",
        "            bernoulli = Bernoulli(gamma)\n",
        "            mask = bernoulli.sample(\n",
        "                (batch_size, channels, height - (self.block_size - 1), width - (self.block_size - 1))).to(DEVICE)\n",
        "            block_mask = self._compute_block_mask(mask)\n",
        "            countM = block_mask.size()[0] * block_mask.size()[1] * block_mask.size()[2] * block_mask.size()[3]\n",
        "            count_ones = block_mask.sum()\n",
        "\n",
        "            return block_mask * x * (countM / count_ones)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def _compute_block_mask(self, mask):\n",
        "        left_padding = int((self.block_size - 1) / 2)\n",
        "        right_padding = int(self.block_size / 2)\n",
        "\n",
        "        non_zero_idxs = mask.nonzero()\n",
        "        nr_blocks = non_zero_idxs.shape[0]\n",
        "\n",
        "        offsets = torch.stack(\n",
        "            [\n",
        "                torch.arange(self.block_size).view(-1, 1).expand(self.block_size, self.block_size).reshape(-1),\n",
        "                torch.arange(self.block_size).repeat(self.block_size),  # - left_padding\n",
        "            ]\n",
        "        ).t().to(DEVICE)\n",
        "        offsets = torch.cat((torch.zeros(self.block_size ** 2, 2).to(DEVICE).long(), offsets.long()), 1)\n",
        "\n",
        "        if nr_blocks > 0:\n",
        "            non_zero_idxs = non_zero_idxs.repeat(self.block_size ** 2, 1)\n",
        "            offsets = offsets.repeat(nr_blocks, 1).view(-1, 4)\n",
        "            offsets = offsets.long()\n",
        "\n",
        "            block_idxs = non_zero_idxs + offsets\n",
        "            padded_mask = F.pad(mask, (left_padding, right_padding, left_padding, right_padding))\n",
        "            padded_mask[block_idxs[:, 0], block_idxs[:, 1], block_idxs[:, 2], block_idxs[:, 3]] = 1.\n",
        "        else:\n",
        "            padded_mask = F.pad(mask, (left_padding, right_padding, left_padding, right_padding))\n",
        "\n",
        "        block_mask = 1 - padded_mask  # [:height, :width]\n",
        "        return block_mask\n",
        "\n",
        "\n",
        "# This ResNet network was designed following the practice of the following papers:\n",
        "# TADAM: Task dependent adaptive metric for improved few-shot learning (Oreshkin et al., in NIPS 2018) and\n",
        "# A Simple Neural Attentive Meta-Learner (Mishra et al., in ICLR 2018).\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"\n",
        "    3x3 convolution with padding\n",
        "    \"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class ResNer12_BasicBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, down_sample=None, drop_rate=0.0, drop_block=False, block_size=1):\n",
        "        super(ResNer12_BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.LeakyReLU(0.1, inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv3x3(planes, planes)\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\n",
        "        self.max_pool = nn.MaxPool2d(stride)\n",
        "        self.down_sample = down_sample\n",
        "        self.stride = stride\n",
        "        self.drop_rate = drop_rate\n",
        "        self.num_batches_tracked = 0\n",
        "        self.drop_block = drop_block\n",
        "        self.block_size = block_size\n",
        "        self.DropBlock = DropBlock(block_size=self.block_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.num_batches_tracked += 1\n",
        "\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.down_sample is not None:\n",
        "            residual = self.down_sample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        out = self.max_pool(out)\n",
        "\n",
        "        if self.drop_rate > 0:\n",
        "            if self.drop_block:\n",
        "                feat_size = out.size()[2]\n",
        "                keep_rate = max(1.0 - self.drop_rate / (20 * 2000) * self.num_batches_tracked, 1.0 - self.drop_rate)\n",
        "                gamma = (1 - keep_rate) / self.block_size ** 2 * feat_size ** 2 / (feat_size - self.block_size + 1) ** 2\n",
        "                out = self.DropBlock(out, gamma=gamma)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, drop_rate, drop_block_size=5):\n",
        "        self.in_planes = 3\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        filters = [64, 64, 64, 64]\n",
        "\n",
        "        self.layer1 = self._make_layer(block, filters[0], stride=2, drop_rate=drop_rate)\n",
        "        self.layer2 = self._make_layer(block, filters[1], stride=2, drop_rate=drop_rate)\n",
        "        self.layer3 = self._make_layer(block, filters[2], stride=2, drop_rate=drop_rate, drop_block=True,\n",
        "                                       block_size=drop_block_size)\n",
        "        self.layer4 = self._make_layer(block, filters[3], stride=2, drop_rate=drop_rate, drop_block=True,\n",
        "                                       block_size=drop_block_size)\n",
        "\n",
        "        self.drop_rate = drop_rate\n",
        "        self.flatten = nn.Sequential(\n",
        "            nn.Flatten(start_dim=1)\n",
        "        )\n",
        "  \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, stride=1, drop_rate=0.0, drop_block=False, block_size=1):\n",
        "        down_sample = None\n",
        "        if stride != 1 or self.in_planes != planes:\n",
        "            down_sample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_planes, planes,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        layers = [block(self.in_planes, planes, stride, down_sample, drop_rate, drop_block, block_size)]\n",
        "        self.in_planes = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.flatten(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet12(drop_rate):\n",
        "    \"\"\"\n",
        "    Constructs a ResNet-12 model\n",
        "    \"\"\"\n",
        "    return ResNet(ResNer12_BasicBlock, drop_rate)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ********************************************* resnet18 *********************************************\n",
        "# ********************************************* resnet18 *********************************************\n",
        "# ********************************************* resnet18 *********************************************\n",
        "# ********************************************* resnet18 *********************************************\n",
        "# ********************************************* resnet18 *********************************************\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNer18_BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResNer18_BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BasicBlockDownSample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(BasicBlockDownSample, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=2, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.down_sample = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        residual = self.down_sample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ResNer18_BasicBlock(64, 64),\n",
        "            ResNer18_BasicBlock(64, 64)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ResNer18_BasicBlock(64, 128),\n",
        "            ResNer18_BasicBlock(128, 128)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ResNer18_BasicBlock(128, 256),\n",
        "            ResNer18_BasicBlock(256, 256)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            ResNer18_BasicBlock(256, 512),\n",
        "            ResNer18_BasicBlock(512, 512)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(start_dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # (200,3,84,84)\n",
        "        x = self.conv1(x)  # (200,64,42,42)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool(x)  # (200,64,21,21)\n",
        "\n",
        "        x = self.layer1(x)  # out (200,64,21,21)\n",
        "        x = self.layer2(x)  # out (200,128,11,11)\n",
        "        x = self.layer3(x)  # out (200,256,6,6)\n",
        "        x = self.layer4(x)  # out (200,512,3,3)\n",
        "        x = self.classifier(x)  # out (200, 4608)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ********************************************* models *********************************************\n",
        "# ********************************************* models *********************************************\n",
        "# ********************************************* models *********************************************\n",
        "# ********************************************* models *********************************************\n",
        "# ********************************************* models *********************************************\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def euclidean_dist(x, y):\n",
        "    \"\"\"\n",
        "    TBD\n",
        "    :param x: size [n_query_total, out_dim=1600] - queries\n",
        "    :param y: size [n_ways, out_dim=1600] - prototypes\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    n = x.size(0)  # total number of query points = n_query_total\n",
        "    m = y.size(0)  # number of classes = n_ways\n",
        "    d = x.size(1)  # dimension of pic embedding = 1600 for mini-ImageNet\n",
        "    if d != y.size(1):\n",
        "        raise ValueError(f'Pic embedding for prototype {y.size(1)} and query {d} data arent equal')\n",
        "\n",
        "    x = x.unsqueeze(1).expand(n, m, d)  # size = [n_query_total, n_ways, 1600]\n",
        "    y = y.unsqueeze(0).expand(n, m, d)  # size = [n_query_total, n_ways, 1600]\n",
        "\n",
        "    return torch.pow(x - y, 2).sum(2)\n",
        "\n",
        "\n",
        "def mahalanobis_dist(x, y):\n",
        "    \"\"\"\n",
        "    TBD\n",
        "    :param x: size [n_query_total, out_dim=1600] - queries\n",
        "    :param y: size [n_ways, out_dim=1600] - prototypes\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    n_queries = 15\n",
        "    n_query_total = x.size(0)\n",
        "    n_ways = y.size(0)  # number of classes = n_ways\n",
        "    res = torch.zeros(n_query_total, n_ways).to(DEVICE)  # size = [n_query_total, n_ways]\n",
        "    queries_per_class = x.split(n_queries, dim=0)  # (10, [15, 1600])\n",
        "    prototypes_per_class = y.split(1, dim=0)  # (10, [1,1600])\n",
        "    batches = int(n_query_total / n_queries)\n",
        "    for class_ndx in range(n_ways):\n",
        "        class_queries = queries_per_class[class_ndx].detach().cpu()\n",
        "        proto = prototypes_per_class[class_ndx]\n",
        "        for query_batch_ndx in range(batches):\n",
        "            query_batch = queries_per_class[query_batch_ndx]\n",
        "            cov_arr = np.cov(class_queries.T)\n",
        "            cov = torch.from_numpy(cov_arr).to(DEVICE)\n",
        "            cov_diag = torch.diag(cov)\n",
        "            cov = torch.diag(cov_diag)\n",
        "\n",
        "            for query_ndx in range(n_queries):\n",
        "                query = query_batch[query_ndx, :]\n",
        "                dist = mahalanobis(proto, query, cov)\n",
        "                q = n_queries * query_batch_ndx + query_ndx\n",
        "                res[q, class_ndx] = dist.item()\n",
        "    return res\n",
        "\n",
        "\n",
        "def mahalanobis(u, v, cov):\n",
        "    delta = (u - v).double()\n",
        "    delta_trans = torch.transpose(delta, 0, 1).double()\n",
        "    cov_inverse = torch.inverse(cov).double()\n",
        "    mult = torch.matmul(delta, cov_inverse)\n",
        "    m = torch.matmul(mult, delta_trans)\n",
        "    return torch.sqrt(m)\n",
        "\n",
        "\n",
        "class ProtoNetSimple(nn.Module):\n",
        "    def __init__(self, num_filters=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.file_name = 'ProtoNetSimple.pt'\n",
        "\n",
        "        self.block1 = self._cnn_block(3, num_filters)\n",
        "        self.block2 = self._cnn_block(num_filters, num_filters)\n",
        "        self.block3 = self._cnn_block(num_filters, num_filters)\n",
        "        self.block4 = self._cnn_block(num_filters, num_filters)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(start_dim=1)\n",
        "        )\n",
        "\n",
        "        self.alpha = nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
        "\n",
        "    @staticmethod\n",
        "    def _cnn_block(in_channels, out_channels):\n",
        "        block = torch.nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        return block\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        vector = self.classifier(x)\n",
        "        return vector\n",
        "\n",
        "    def loss(self, episode, image_data):\n",
        "        support_data = episode.get_support_sample(image_data)\n",
        "        query_data = episode.get_query_sample(image_data)\n",
        "\n",
        "        xs = Variable(support_data)\n",
        "        # xs size = [n_ways, n_shots, channels=3, width=84, height=84]\n",
        "        xq = Variable(query_data)\n",
        "        # xq size = [n_ways, n_query_points, channels=3, width=84, height=84]\n",
        "\n",
        "        n_class = xs.size(0)\n",
        "        if xq.size(0) != n_class:\n",
        "            raise ValueError(f'Number of classes for support {xs.size(0)} and query {xq.size(0)} data is not equal')\n",
        "        n_support = xs.size(1)\n",
        "        n_query = xq.size(1)\n",
        "\n",
        "        target_indices = torch.arange(0, n_class).view(n_class, 1, 1).expand(n_class, n_query, 1).long().to(DEVICE)\n",
        "        target_indices = Variable(target_indices, requires_grad=False)\n",
        "\n",
        "        support_pic_size = xs.size()[2:]\n",
        "        query_pic_size = xq.size()[2:]\n",
        "        if query_pic_size != support_pic_size:\n",
        "            raise ValueError(f'Pic sizes for support {support_pic_size} and query {query_pic_size} data arent equal')\n",
        "        n_support_total = n_class * n_support\n",
        "        n_query_total = n_class * n_query\n",
        "        xs_view = xs.view(n_support_total, *support_pic_size)\n",
        "        xq_view = xq.view(n_query_total, *query_pic_size)\n",
        "        x = torch.cat([xs_view, xq_view], 0).float().to(DEVICE)  # input for the model\n",
        "        # x = has dimension of [n_support_total + n_query_total, channels=3, width=84, height=84]\n",
        "\n",
        "        z = self.forward(x)  # output with dimension [n_support_total + n_query_total, 1600]\n",
        "        z_dim = z.size(-1)\n",
        "\n",
        "        z_support = z[:n_support_total]  # size = [n_support_total, 1600]\n",
        "        # prototype = average all embeddings from support set\n",
        "        prototypes_per_class = z_support.view(n_class, n_support, z_dim).mean(1)  # size = [n_class, 1600]\n",
        "        query_vectors = z[n_support_total:]  # size = [n_query_total, 1600]\n",
        "\n",
        "        dists_per_class = euclidean_dist(query_vectors, prototypes_per_class)  # size = [n_query_total, n_ways]\n",
        "        # dists_per_class = mahalanobis_dist(query_vectors, prototypes_per_class)  # size = [n_query_total, n_ways]\n",
        "\n",
        "        # alpha used here ----------------------------------------------------------\n",
        "        # alpha parameter is used to scale the distance metric to obtain better results\n",
        "        if self.alpha is not None:\n",
        "            dists_per_class = torch.mul(self.alpha, dists_per_class)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        log_p_y = torch.nn.functional.log_softmax(-dists_per_class, dim=1).view(n_class, n_query, -1)  # log(p(y=k|x))\n",
        "        # log_p_y = size [n_class = n_ways, n_query, n_class]\n",
        "        loss_per_query = -log_p_y.gather(2, target_indices).squeeze().view(-1)  # size = [n_query_total]\n",
        "        loss_val = loss_per_query.mean()  # average loss for all queries\n",
        "        _, y_hat = log_p_y.max(dim=2)  # returns tuple (max values, argmax indices)\n",
        "        # y_hat size = [n_class, n_query]\n",
        "\n",
        "        # calculate accuracy = number of matches between y_hat indices and ground truth target_indices\n",
        "        acc_val = torch.eq(y_hat, target_indices.squeeze()).float().mean()\n",
        "\n",
        "        return loss_val, {\n",
        "            'loss': loss_val.item(),\n",
        "            'acc': acc_val.item()\n",
        "        }\n",
        "\n",
        "    def save(self, report_folder):\n",
        "        model_file = os.path.join(report_folder, self.file_name)\n",
        "        torch.save(self.state_dict(), model_file)\n",
        "\n",
        "\n",
        "class ProtoNetComplex(ProtoNetSimple):\n",
        "\n",
        "    def __init__(self, num_filters=64):\n",
        "        super().__init__(num_filters)\n",
        "\n",
        "        self.file_name = 'ProtoNetComplex.pt'\n",
        "\n",
        "        self.block1 = self._cnn_block(3, num_filters)\n",
        "        self.block2 = self._cnn_block(num_filters, num_filters)\n",
        "        self.block3 = self._cnn_block(num_filters, num_filters)\n",
        "        self.block4 = self._cnn_block(num_filters, num_filters)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Linear(1600, 1600),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1600, 128)\n",
        "        )\n",
        "\n",
        "\n",
        "class ProtoNetRes18(ProtoNetSimple):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.res_net = ResNet18()\n",
        "        self.file_name = 'ProtoNetRes18.pt'\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.res_net.forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ProtoNetRes12(ProtoNetSimple):\n",
        "    def __init__(self, drop_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.res_net = resnet12(drop_rate=drop_rate)\n",
        "        self.file_name = 'ProtoNetRes12.pt'\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.res_net.forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNetSimple(ProtoNetSimple):\n",
        "    \"\"\"\n",
        "    Densely Connected CNN\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_filters=64):\n",
        "        super().__init__()\n",
        "        self.file_name = 'ResNetSimple.pt'\n",
        "        self.maxPool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.alpha = nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.flatten = nn.Sequential(\n",
        "            nn.Flatten(start_dim=1)\n",
        "        )\n",
        "        self.BN = nn.BatchNorm2d(num_filters)\n",
        "\n",
        "        self.block1 = self._cnn_block(3, num_filters)\n",
        "        self.blockRes1 = self._cnn_block_Res(3, num_filters)\n",
        "\n",
        "        self.block2 = self._cnn_block(num_filters, num_filters)\n",
        "        self.blockRes2 = self._cnn_block_Res(num_filters, num_filters)\n",
        "\n",
        "    @staticmethod\n",
        "    def _cnn_block(in_channels, out_channels):\n",
        "        block = torch.nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "        )\n",
        "        return block\n",
        "\n",
        "    @staticmethod\n",
        "    def _cnn_block_Res(in_channels, out_channels):\n",
        "        block = torch.nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        return block\n",
        "\n",
        "    def forward(self, x):\n",
        "        # **************************   block 1   *********************************\n",
        "        # on the original input of the block we run a conv layer and Batch normalization\n",
        "        residual = x\n",
        "        residual = self.blockRes1(residual)\n",
        "\n",
        "        # on the original input run conv -> relu -> conv -> relu -> conv\n",
        "        # and then sum the outputs of the above two blocks and then run relu and MP\n",
        "        # run the same architecture on the other 3 blocks\n",
        "        x = self.block1(x)\n",
        "        x = x + residual\n",
        "        x = self.relu(x)\n",
        "        x = self.maxPool(x)\n",
        "\n",
        "        # **************************   block 2   *********************************\n",
        "        residual = x\n",
        "        residual = self.blockRes2(residual)\n",
        "        x = self.block2(x)\n",
        "        x = x + residual\n",
        "        x = self.relu(x)\n",
        "        x = self.maxPool(x)\n",
        "\n",
        "        # **************************   block 3   *********************************\n",
        "        residual = x\n",
        "        residual = self.blockRes2(residual)\n",
        "        x = self.block2(x)\n",
        "        x = x + residual\n",
        "        x = self.relu(x)\n",
        "        x = self.maxPool(x)\n",
        "\n",
        "        # **************************   block 4   *********************************\n",
        "        residual = x\n",
        "        residual = self.blockRes2(residual)\n",
        "        x = self.block2(x)\n",
        "        x = x + residual\n",
        "        x = self.relu(x)\n",
        "        x = self.maxPool(x)\n",
        "\n",
        "        vector = self.flatten(x)\n",
        "        return vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ********************************************* parameters *********************************************\n",
        "# ********************************************* parameters *********************************************\n",
        "# ********************************************* parameters *********************************************\n",
        "# ********************************************* parameters *********************************************\n",
        "# ********************************************* parameters *********************************************\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Episode(object):\n",
        "    \"\"\"\n",
        "    Class representation for a single episode\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.indexes = {}  # class_name -> (support pics indices, query pics indices)\n",
        "        self.SUPPORT_DATA_NDX = 0\n",
        "        self.QUERY_DATA_NDX = 1\n",
        "\n",
        "    def get_support_sample(self, image_data):\n",
        "        return self._get_data_sample(image_data, self.SUPPORT_DATA_NDX)\n",
        "\n",
        "    def get_query_sample(self, image_data):\n",
        "        return self._get_data_sample(image_data, self.QUERY_DATA_NDX)\n",
        "\n",
        "    def add_indices(self, class_name, support_ndxs, query_ndxs):\n",
        "        self.indexes[class_name] = (support_ndxs, query_ndxs)\n",
        "\n",
        "    def _get_data_sample(self, image_data, data_index):\n",
        "        all_samples = []\n",
        "        for class_name in self.indexes.keys():\n",
        "            ndxs = self.indexes[class_name][data_index]\n",
        "            sample_indices = torch.tensor(ndxs)\n",
        "            sample = torch.index_select(image_data, 0, sample_indices)\n",
        "            sample = sample.transpose(1, 3).transpose(2, 3)\n",
        "            all_samples.append(sample)\n",
        "        result = torch.stack(all_samples, dim=0)\n",
        "        return result\n",
        "\n",
        "\n",
        "class RunParameters(object):\n",
        "    \"\"\"\n",
        "    Use this class for train + validation run\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.epochs = 0\n",
        "        self.loss = None\n",
        "        self.optimizer = None\n",
        "        self.scheduler = None\n",
        "        self.report_path = None  # path where to save PT file and logs\n",
        "        self.train_data = None\n",
        "        self.val_data = None\n",
        "        self.episodes_per_epoch = 0\n",
        "        self.patience = 0\n",
        "        self.n_ways = 0\n",
        "        self.n_support_examples = 0\n",
        "        self.n_query_examples = 0\n",
        "\n",
        "    def __str__(self):\n",
        "        return '--------Run params--------\\n' \\\n",
        "               f'Epochs: {self.epochs}\\n' \\\n",
        "               f'Loss: {self.loss}\\n' \\\n",
        "               f'Optimizer:{self.optimizer}\\n' \\\n",
        "               f'Scheduler:{self.scheduler}\\n' \\\n",
        "               f'report_path:{self.report_path}\\n' \\\n",
        "               f'episodes_per_epoch:{self.episodes_per_epoch}\\n' \\\n",
        "               f'patience:{self.patience}\\n' \\\n",
        "               f'n_ways:{self.n_ways}\\n' \\\n",
        "               f'n_support_examples:{self.n_support_examples}\\n' \\\n",
        "               f'n_query_examples:{self.n_query_examples}\\n' \\\n",
        "               '-----------------------------\\n'\n",
        "\n",
        "\n",
        "class TrainResult(object):\n",
        "    \"\"\"\n",
        "    Result of train + validation run\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train_loss_per_epoch = []\n",
        "        self.train_accuracy_per_epoch = []\n",
        "        self.validation_loss_per_epoch = []\n",
        "        self.validation_accuracy_per_epoch = []\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def train_loss_min(self):\n",
        "        return min(self.train_loss_per_epoch)\n",
        "\n",
        "    def valid_loss_min(self):\n",
        "        return min(self.validation_loss_per_epoch)\n",
        "\n",
        "    def train_accuracy(self):\n",
        "        return max(self.train_accuracy_per_epoch)\n",
        "\n",
        "    def validation_accuracy(self):\n",
        "        return max(self.validation_accuracy_per_epoch)\n",
        "\n",
        "    def __str__(self):\n",
        "        return '--------Run result--------\\n' \\\n",
        "               f'Train Loss: {self.train_loss_min()}\\n' \\\n",
        "               f'Train Acc: {self.train_accuracy()}\\n' \\\n",
        "               f'Valid Loss:{self.valid_loss_min()}\\n' \\\n",
        "               f'Valid Acc:{self.validation_accuracy()}\\n' \\\n",
        "               f'Best Epoch:{self.best_epoch}\\n' \\\n",
        "               '-----------------------------\\n'\n",
        "\n",
        "\n",
        "class TestResult:\n",
        "    \"\"\"\n",
        "    Used for test evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.acc = 0  # accuracy\n",
        "        self.loss = 0  # loss\n",
        "\n",
        "    def __str__(self):\n",
        "        return '--------Test result--------\\n' \\\n",
        "               f'Test Acc: {self.acc}\\n' \\\n",
        "               f'Test Loss:{self.loss}\\n' \\\n",
        "               '-----------------------------\\n'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ********************************************* utils *********************************************\n",
        "# ********************************************* utils *********************************************\n",
        "# ********************************************* utils *********************************************\n",
        "# ********************************************* utils *********************************************\n",
        "# ********************************************* utils *********************************************\n",
        "\n",
        "\n",
        "\n",
        "def setup_reports():\n",
        "    \"\"\"\n",
        "    Create setup for logger - both to file and to console\n",
        "    Also model will be saved in the same folder with logs\n",
        "    \"\"\"\n",
        "    report_root = 'reports'\n",
        "    if not os.path.exists(report_root):\n",
        "        os.makedirs(report_root)\n",
        "    report_folder = f'{TIMESTAMP}_report'\n",
        "    report_path = os.path.join(report_root, report_folder)\n",
        "    if not os.path.exists(report_path):\n",
        "        os.makedirs(report_path)\n",
        "\n",
        "    logging.getLogger('matplotlib.font_manager').disabled = True\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
        "    log_file = f'{TIMESTAMP}.log'\n",
        "    output_file_handler = logging.FileHandler(f'{report_path}\\{log_file}')\n",
        "    logger.addHandler(output_file_handler)\n",
        "    logger.addHandler(stdout_handler)\n",
        "    return report_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ********************************************* engine *********************************************\n",
        "# ********************************************* engine *********************************************\n",
        "# ********************************************* engine *********************************************\n",
        "# ********************************************* engine *********************************************\n",
        "# ********************************************* engine *********************************************\n",
        "\n",
        "\n",
        "def choose_episode_classes(class_dict, n_ways):\n",
        "    \"\"\"\n",
        "    Choose which classes will participate in the episode\n",
        "    :param class_dict: dictionary of all classes\n",
        "    :param n_ways: number of classes per episode\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # select classes for episode (random uniform)\n",
        "    all_classes = class_dict.keys()\n",
        "    classes = random.sample(all_classes, n_ways)\n",
        "    return classes\n",
        "\n",
        "\n",
        "def create_episodes(class_dict, n_episodes, n_ways, n_supports, n_queries):\n",
        "    \"\"\"\n",
        "    TBD\n",
        "    :param class_dict:\n",
        "    :param n_episodes:\n",
        "    :param n_ways:\n",
        "    :param n_supports:\n",
        "    :param n_queries:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    episodes = []\n",
        "\n",
        "    for e in range(n_episodes):\n",
        "        try:\n",
        "            classes = choose_episode_classes(class_dict, n_ways)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "        episode = Episode()\n",
        "        for c in classes:\n",
        "            class_ndxs = class_dict[c]\n",
        "            selected_ndxs = random.sample(class_ndxs, n_supports + n_queries)\n",
        "\n",
        "            support_ndxs = selected_ndxs[:n_supports]\n",
        "            query_ndxs = selected_ndxs[n_supports:]\n",
        "            episode.add_indices(c, support_ndxs, query_ndxs)\n",
        "\n",
        "        episodes.append(episode)\n",
        "\n",
        "    return episodes\n",
        "\n",
        "\n",
        "def train(parameters, log):\n",
        "    \"\"\"\n",
        "    Run training procedure\n",
        "    :param log: logger to file / console\n",
        "    :param parameters: training parameters context\n",
        "    :return: train result\n",
        "    \"\"\"\n",
        "    log('Train started')\n",
        "    train_res = TrainResult()\n",
        "\n",
        "    best_loss = math.inf\n",
        "    epochs_since_best = 0\n",
        "    for ep in range(parameters.epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        train_loop(ep, parameters, train_res, log)\n",
        "        validation_loop(parameters, train_res, log, f\"validation\")\n",
        "        if parameters.scheduler is not None:\n",
        "            parameters.scheduler.step()\n",
        "\n",
        "        # early stopping\n",
        "        epoch_loss = train_res.validation_loss_per_epoch[-1].item()\n",
        "        if epoch_loss < best_loss:\n",
        "            epochs_since_best = 0\n",
        "            best_loss = epoch_loss\n",
        "            parameters.model.save(parameters.report_path)\n",
        "            log('best model saved')\n",
        "        else:\n",
        "            epochs_since_best = epochs_since_best + 1\n",
        "        if epochs_since_best > parameters.patience:\n",
        "            # update result and get out\n",
        "            train_res.best_epoch = ep - parameters.patience\n",
        "            return train_res\n",
        "\n",
        "        elapsed_time = time.time() - epoch_start_time\n",
        "        log(f'time: {elapsed_time:5.2f} sec')\n",
        "\n",
        "    return train_res\n",
        "\n",
        "\n",
        "def train_loop(ep, parameters, train_res, log):\n",
        "    \"\"\"\n",
        "    TBD\n",
        "    :param ep:\n",
        "    :param parameters:\n",
        "    :param train_res:\n",
        "    :param log:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    model = parameters.model\n",
        "    optimizer = parameters.optimizer\n",
        "    n_ways = parameters.n_ways\n",
        "    n_episodes = parameters.episodes_per_epoch\n",
        "    n_supports = parameters.n_support_examples\n",
        "    n_queries = parameters.n_query_examples\n",
        "    train_image_data = torch.from_numpy(parameters.train_data['image_data'])\n",
        "    train_class_dict = parameters.train_data['class_dict']\n",
        "\n",
        "    model.train()\n",
        "    train_batch_losses = []\n",
        "    train_batch_accuracies = []\n",
        "    train_episodes = create_episodes(train_class_dict, n_episodes, n_ways, n_supports, n_queries)\n",
        "    log(f'Epoch {ep + 1}')\n",
        "    for episode in tqdm(train_episodes, desc=f\"Epoch {ep + 1}/{parameters.epochs} train\"):\n",
        "        optimizer.zero_grad()\n",
        "        loss, output = model.loss(episode, train_image_data)\n",
        "        train_batch_losses.append(output['loss'])\n",
        "        train_batch_accuracies.append(output['acc'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    epoch_train_loss = torch.mean(torch.tensor(train_batch_losses))\n",
        "    epoch_train_acc = torch.mean(torch.tensor(train_batch_accuracies))\n",
        "    log(f'train loss: {epoch_train_loss:.6f}')\n",
        "    log(f'train acc: {epoch_train_acc:.6f}')\n",
        "    # update train losses in result here\n",
        "    train_res.train_accuracy_per_epoch.append(epoch_train_acc)\n",
        "    train_res.train_loss_per_epoch.append(epoch_train_loss)\n",
        "\n",
        "\n",
        "def validation_loop(parameters, train_res, log, desc):\n",
        "    \"\"\"\n",
        "    TBD\n",
        "    :param desc:\n",
        "    :param parameters:\n",
        "    :param train_res:\n",
        "    :param log:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    model = parameters.model\n",
        "    n_ways = parameters.n_ways\n",
        "    n_episodes = parameters.episodes_per_epoch\n",
        "    n_supports = parameters.n_support_examples\n",
        "    n_queries = parameters.n_query_examples\n",
        "    val_image_data = torch.from_numpy(parameters.val_data['image_data'])\n",
        "    val_class_dict = parameters.val_data['class_dict']\n",
        "\n",
        "    model.eval()\n",
        "    val_batch_losses = []\n",
        "    val_batch_accuracies = []\n",
        "    with torch.no_grad():\n",
        "        val_episodes = create_episodes(val_class_dict, n_episodes, n_ways, n_supports, n_queries)\n",
        "        for episode in tqdm(val_episodes, desc=desc):\n",
        "            _, output = model.loss(episode, val_image_data)\n",
        "            val_batch_losses.append(output['loss'])\n",
        "            val_batch_accuracies.append(output['acc'])\n",
        "    epoch_val_loss = torch.mean(torch.tensor(val_batch_losses))\n",
        "    epoch_val_acc = torch.mean(torch.tensor(val_batch_accuracies))\n",
        "    log(f'{desc} loss: {epoch_val_loss:.6f}')\n",
        "    log(f'{desc} acc: {epoch_val_acc:.6f}')\n",
        "    # update losses and accuracies in result here\n",
        "    train_res.validation_accuracy_per_epoch.append(epoch_val_acc)\n",
        "    train_res.validation_loss_per_epoch.append(epoch_val_loss)\n",
        "\n",
        "\n",
        "def test(parameters, log):\n",
        "    \"\"\"\n",
        "    TBD\n",
        "    :param parameters:\n",
        "    :param log:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    log('Test started')\n",
        "    test_res = TrainResult()\n",
        "    epoch_start_time = time.time()\n",
        "    validation_loop(parameters, test_res, log, f\"Test\")\n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    log(f'time: {elapsed_time:5.2f} sec')\n",
        "    result = TestResult()\n",
        "    result.acc = test_res.validation_accuracy_per_epoch[-1].item()\n",
        "    result.loss = test_res.validation_loss_per_epoch[-1].item()\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ********************************************* run_train *********************************************\n",
        "# ********************************************* run_train *********************************************\n",
        "# ********************************************* run_train *********************************************\n",
        "# ********************************************* run_train *********************************************\n",
        "# ********************************************* run_train *********************************************\n",
        "\n",
        "\n",
        "\n",
        "def load_train_data():\n",
        "    \"\"\"\n",
        "    Load train split data from Pickle file\n",
        "    :return: data dictionary\n",
        "    \"\"\"\n",
        "    with open(os.path.join(DATA_DIR, TRAIN_DATA_FILE), \"rb\") as data_file:\n",
        "        data = pickle.load(data_file)\n",
        "    log(f'Loaded train data: {TRAIN_DATA_FILE}')\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_validation_data():\n",
        "    \"\"\"\n",
        "    Load validation split data from Pickle file\n",
        "    :return: data dictionary\n",
        "    \"\"\"\n",
        "    with open(os.path.join(DATA_DIR, VALID_DATA_FILE), \"rb\") as data_file:\n",
        "        data = pickle.load(data_file)\n",
        "    log(f'Loaded validation data: {VALID_DATA_FILE}')\n",
        "    return data\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    TBD\n",
        "    :return: TBD\n",
        "    \"\"\"\n",
        "    report_path = setup_reports()\n",
        "    log(DEVICE)\n",
        "\n",
        "    num_filters = 64\n",
        "    log(f'Num Filters: {num_filters}')\n",
        "    drop_rate = 0.2\n",
        "    log(f'Drop block rate: {drop_rate}')\n",
        "\n",
        "    model = ProtoNetRes12().to(DEVICE)\n",
        "\n",
        "    log(model)\n",
        "\n",
        "    log(f'Alpha:{list(model.parameters())[0].item()}')\n",
        "\n",
        "    lr = 0.002\n",
        "    log(f'LR:{lr}')\n",
        "    step = 50\n",
        "    log(f'LR Step:{step}')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step, gamma=0.5, last_epoch=-1)\n",
        "\n",
        "    # load train & validation data\n",
        "    train_data = load_train_data()\n",
        "    val_data = load_validation_data()\n",
        "    train_params = RunParameters()\n",
        "    train_params.epochs = 10000  # max number of epochs\n",
        "    train_params.optimizer = optimizer\n",
        "    train_params.scheduler = scheduler\n",
        "    train_params.train_data = train_data\n",
        "    train_params.val_data = val_data\n",
        "\n",
        "    train_params.n_ways = 5  # ways\n",
        "    train_params.patience = 200\n",
        "    train_params.episodes_per_epoch = 100\n",
        "    train_params.n_support_examples = 5  # shots\n",
        "    train_params.n_query_examples = 15\n",
        "    train_params.model = model\n",
        "    train_params.report_path = report_path\n",
        "    log(train_params)\n",
        "    train_res = train(train_params, log)\n",
        "    log(train_res)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJq4jdPUVdgn",
        "outputId": "03eaa5f2-bd0f-4dce-c09a-1e6c07e68a6d"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    print('OK')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Num Filters: 64\n",
            "Drop block rate: 0.2\n",
            "ProtoNetRes12(\n",
            "  (block1): Sequential(\n",
            "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block2): Sequential(\n",
            "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block3): Sequential(\n",
            "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block4): Sequential(\n",
            "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (res_net): ResNet(\n",
            "    (layer1): Sequential(\n",
            "      (0): ResNer12_BasicBlock(\n",
            "        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (down_sample): Sequential(\n",
            "          (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (DropBlock): DropBlock()\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): ResNer12_BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (down_sample): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (DropBlock): DropBlock()\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): ResNer12_BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (down_sample): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (DropBlock): DropBlock()\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): ResNer12_BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (down_sample): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (DropBlock): DropBlock()\n",
            "      )\n",
            "    )\n",
            "    (flatten): Sequential(\n",
            "      (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Alpha:1.0\n",
            "LR:0.002\n",
            "LR Step:50\n",
            "Loaded train data: mini-imagenet-cache-train.pkl\n",
            "Loaded validation data: mini-imagenet-cache-val.pkl\n",
            "--------Run params--------\n",
            "Epochs: 10000\n",
            "Loss: None\n",
            "Optimizer:Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.002\n",
            "    lr: 0.002\n",
            "    weight_decay: 0\n",
            ")\n",
            "Scheduler:<torch.optim.lr_scheduler.StepLR object at 0x7f978689e4d0>\n",
            "report_path:reports/2021_08_31-09_57_06_report\n",
            "episodes_per_epoch:100\n",
            "patience:200\n",
            "n_ways:5\n",
            "n_support_examples:5\n",
            "n_query_examples:15\n",
            "-----------------------------\n",
            "\n",
            "Train started\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/10000 train:   0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Epoch 1/10000 train: 100%|| 100/100 [00:19<00:00,  5.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 41.196465\n",
            "train acc: 0.431733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 7.933660\n",
            "validation acc: 0.400667\n",
            "best model saved\n",
            "time: 26.66 sec\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2/10000 train: 100%|| 100/100 [00:20<00:00,  4.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 3.996119\n",
            "train acc: 0.442267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 2.310611\n",
            "validation acc: 0.408800\n",
            "best model saved\n",
            "time: 27.02 sec\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3/10000 train: 100%|| 100/100 [00:20<00:00,  4.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.792603\n",
            "train acc: 0.441733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:07<00:00, 14.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.526839\n",
            "validation acc: 0.416800\n",
            "best model saved\n",
            "time: 27.71 sec\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 4/10000 train: 100%|| 100/100 [00:21<00:00,  4.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.448980\n",
            "train acc: 0.461333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.450800\n",
            "validation acc: 0.413867\n",
            "best model saved\n",
            "time: 28.06 sec\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 5/10000 train: 100%|| 100/100 [00:21<00:00,  4.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.303084\n",
            "train acc: 0.479067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.386311\n",
            "validation acc: 0.437200\n",
            "best model saved\n",
            "time: 28.61 sec\n",
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 6/10000 train: 100%|| 100/100 [00:21<00:00,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.296773\n",
            "train acc: 0.489333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.350626\n",
            "validation acc: 0.446133\n",
            "best model saved\n",
            "time: 28.21 sec\n",
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7/10000 train: 100%|| 100/100 [00:21<00:00,  4.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.263672\n",
            "train acc: 0.493600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.341240\n",
            "validation acc: 0.450533\n",
            "best model saved\n",
            "time: 28.15 sec\n",
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 8/10000 train: 100%|| 100/100 [00:21<00:00,  4.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.282258\n",
            "train acc: 0.479467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.355444\n",
            "validation acc: 0.439600\n",
            "time: 28.35 sec\n",
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9/10000 train: 100%|| 100/100 [00:21<00:00,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.248510\n",
            "train acc: 0.492667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.335311\n",
            "validation acc: 0.446533\n",
            "best model saved\n",
            "time: 28.25 sec\n",
            "Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10/10000 train: 100%|| 100/100 [00:21<00:00,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.237673\n",
            "train acc: 0.502400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.306427\n",
            "validation acc: 0.464800\n",
            "best model saved\n",
            "time: 28.20 sec\n",
            "Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11/10000 train: 100%|| 100/100 [00:21<00:00,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.215767\n",
            "train acc: 0.516133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.296852\n",
            "validation acc: 0.468533\n",
            "best model saved\n",
            "time: 28.27 sec\n",
            "Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12/10000 train: 100%|| 100/100 [00:21<00:00,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.210047\n",
            "train acc: 0.518400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.324509\n",
            "validation acc: 0.451067\n",
            "time: 28.23 sec\n",
            "Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.209162\n",
            "train acc: 0.515333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.313265\n",
            "validation acc: 0.455200\n",
            "time: 28.28 sec\n",
            "Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 14/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.195520\n",
            "train acc: 0.525200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.295556\n",
            "validation acc: 0.465733\n",
            "best model saved\n",
            "time: 28.31 sec\n",
            "Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 15/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.180341\n",
            "train acc: 0.523467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.293404\n",
            "validation acc: 0.472400\n",
            "best model saved\n",
            "time: 28.33 sec\n",
            "Epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 16/10000 train: 100%|| 100/100 [00:21<00:00,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.179506\n",
            "train acc: 0.535733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.278445\n",
            "validation acc: 0.474000\n",
            "best model saved\n",
            "time: 28.35 sec\n",
            "Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 17/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.156710\n",
            "train acc: 0.540267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.274048\n",
            "validation acc: 0.476933\n",
            "best model saved\n",
            "time: 28.32 sec\n",
            "Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 18/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.159234\n",
            "train acc: 0.542533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.237698\n",
            "validation acc: 0.500533\n",
            "best model saved\n",
            "time: 28.31 sec\n",
            "Epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 19/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.159303\n",
            "train acc: 0.537333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.295219\n",
            "validation acc: 0.491067\n",
            "time: 28.29 sec\n",
            "Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 20/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.130165\n",
            "train acc: 0.556400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.279875\n",
            "validation acc: 0.473733\n",
            "time: 28.27 sec\n",
            "Epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 21/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.116047\n",
            "train acc: 0.566533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.237519\n",
            "validation acc: 0.502667\n",
            "best model saved\n",
            "time: 28.31 sec\n",
            "Epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 22/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.096828\n",
            "train acc: 0.570933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.230772\n",
            "validation acc: 0.512667\n",
            "best model saved\n",
            "time: 28.30 sec\n",
            "Epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 23/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.138094\n",
            "train acc: 0.553200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.215932\n",
            "validation acc: 0.510533\n",
            "best model saved\n",
            "time: 28.29 sec\n",
            "Epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 24/10000 train: 100%|| 100/100 [00:21<00:00,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.096061\n",
            "train acc: 0.573733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.213117\n",
            "validation acc: 0.511733\n",
            "best model saved\n",
            "time: 28.31 sec\n",
            "Epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 25/10000 train: 100%|| 100/100 [00:21<00:00,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.077407\n",
            "train acc: 0.580533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.231584\n",
            "validation acc: 0.504000\n",
            "time: 28.32 sec\n",
            "Epoch 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 26/10000 train: 100%|| 100/100 [00:21<00:00,  4.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.122819\n",
            "train acc: 0.556000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:07<00:00, 14.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.253557\n",
            "validation acc: 0.496667\n",
            "time: 28.48 sec\n",
            "Epoch 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 27/10000 train: 100%|| 100/100 [00:21<00:00,  4.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.075812\n",
            "train acc: 0.585067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.192590\n",
            "validation acc: 0.518933\n",
            "best model saved\n",
            "time: 28.46 sec\n",
            "Epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 28/10000 train: 100%|| 100/100 [00:21<00:00,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.101419\n",
            "train acc: 0.565067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.209191\n",
            "validation acc: 0.520000\n",
            "time: 28.30 sec\n",
            "Epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 29/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.067651\n",
            "train acc: 0.577200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.247000\n",
            "validation acc: 0.501600\n",
            "time: 28.31 sec\n",
            "Epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 30/10000 train: 100%|| 100/100 [00:21<00:00,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.078936\n",
            "train acc: 0.575067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.214851\n",
            "validation acc: 0.509467\n",
            "time: 28.27 sec\n",
            "Epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 31/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.086512\n",
            "train acc: 0.577600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.261733\n",
            "validation acc: 0.493867\n",
            "time: 28.29 sec\n",
            "Epoch 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 32/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.041196\n",
            "train acc: 0.597600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.255529\n",
            "validation acc: 0.506933\n",
            "time: 28.25 sec\n",
            "Epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 33/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.056150\n",
            "train acc: 0.588533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.214480\n",
            "validation acc: 0.509200\n",
            "time: 28.25 sec\n",
            "Epoch 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 34/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.059453\n",
            "train acc: 0.591733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.178192\n",
            "validation acc: 0.532933\n",
            "best model saved\n",
            "time: 28.30 sec\n",
            "Epoch 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 35/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.050289\n",
            "train acc: 0.588667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.200378\n",
            "validation acc: 0.525600\n",
            "time: 28.25 sec\n",
            "Epoch 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 36/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.045259\n",
            "train acc: 0.596133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.169670\n",
            "validation acc: 0.526533\n",
            "best model saved\n",
            "time: 28.28 sec\n",
            "Epoch 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 37/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.044351\n",
            "train acc: 0.594400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.182223\n",
            "validation acc: 0.527467\n",
            "time: 28.27 sec\n",
            "Epoch 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 38/10000 train: 100%|| 100/100 [00:21<00:00,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.035925\n",
            "train acc: 0.601467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.174496\n",
            "validation acc: 0.531200\n",
            "time: 28.28 sec\n",
            "Epoch 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 39/10000 train: 100%|| 100/100 [00:21<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 0.993015\n",
            "train acc: 0.614933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.186807\n",
            "validation acc: 0.521600\n",
            "time: 28.32 sec\n",
            "Epoch 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 40/10000 train: 100%|| 100/100 [00:21<00:00,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.011571\n",
            "train acc: 0.606133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "validation: 100%|| 100/100 [00:06<00:00, 14.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss: 1.161065\n",
            "validation acc: 0.536667\n",
            "best model saved\n",
            "time: 28.44 sec\n",
            "Epoch 41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 41/10000 train:  65%|   | 65/100 [00:13<00:07,  4.65it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PstYc4vGVdy-"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}